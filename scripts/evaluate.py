#!/usr/bin/env python3
"""
Draft Evaluator for GrantOps.

Evaluates a draft section across different dimensions:
- style: Writing quality and style guide adherence
- logic: Requirements coverage and internal consistency
- alignment: Project narrative coherence and cross-section consistency

Usage:
    python scripts/evaluate.py <section_id> --mode <style|logic|alignment>
"""

import argparse
import sys
from pathlib import Path

# Add scripts directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from core.context import build_evaluate_context, read_file, list_sections, get_repo_root
from core.llm import call_llm


VALID_MODES = ["style", "logic", "alignment"]


def validate_section(section_id: str) -> bool:
    """Check if a section exists and has a draft."""
    draft_path = get_repo_root() / "application" / "sections" / section_id / "draft.md"
    return draft_path.exists()


def evaluate_draft(section_id: str, mode: str) -> str:
    """
    Evaluate a draft in the specified mode.

    Args:
        section_id: The section to evaluate
        mode: Evaluation mode (style, logic, or alignment)

    Returns:
        Evaluation feedback
    """
    # Build the context
    context = build_evaluate_context(section_id, mode)

    # Extract system prompt from context (first part before ---)
    parts = context.split("\n\n---\n\n")
    system_prompt = parts[0] if parts else ""
    user_prompt = "\n\n---\n\n".join(parts[1:]) if len(parts) > 1 else context

    # Add explicit instruction
    user_prompt += f"\n\n---\n\nPlease evaluate this draft using {mode} criteria."

    # Call the LLM
    evaluation = call_llm(
        agent_name="evaluator",
        prompt=user_prompt,
        system_prompt=system_prompt,
    )

    return evaluation


def format_evaluation_report(section_id: str, mode: str, evaluation: str) -> str:
    """
    Format the evaluation as a markdown report.

    Args:
        section_id: The evaluated section
        mode: Evaluation mode used
        evaluation: Raw evaluation from LLM

    Returns:
        Formatted markdown report
    """
    mode_titles = {
        "style": "Style & Clarity",
        "logic": "Requirements & Logic",
        "alignment": "Project Alignment",
    }

    return f"""## Evaluation Report: {mode_titles.get(mode, mode)}

**Section:** `{section_id}`
**Mode:** {mode}

---

{evaluation}

---
*Generated by GrantOps evaluate workflow*
"""


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate a grant application section draft"
    )
    parser.add_argument(
        "section_id",
        help="Section identifier (e.g., 'project_narrative')"
    )
    parser.add_argument(
        "--mode",
        choices=VALID_MODES,
        required=True,
        help="Evaluation mode: style, logic, or alignment"
    )
    parser.add_argument(
        "--output",
        choices=["stdout", "file", "comment"],
        default="stdout",
        help="Output destination (default: stdout)"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="List available sections and exit"
    )

    args = parser.parse_args()

    # List sections mode
    if args.list:
        sections = list_sections()
        if sections:
            print("Available sections with drafts:")
            for s in sections:
                if validate_section(s):
                    print(f"  - {s}")
        else:
            print("No sections found with drafts.")
        return 0

    # Validate section has a draft
    if not validate_section(args.section_id):
        print(f"Error: No draft found for section '{args.section_id}'.")
        print(f"Expected file: application/sections/{args.section_id}/draft.md")
        print("Run the draft workflow first, or create the draft manually.")
        return 1

    print(f"Evaluating section '{args.section_id}' in {args.mode} mode...")

    # Run evaluation
    try:
        evaluation = evaluate_draft(args.section_id, args.mode)
    except Exception as e:
        print(f"Error during evaluation: {e}")
        return 1

    # Format the report
    report = format_evaluation_report(args.section_id, args.mode, evaluation)

    # Output handling
    if args.output == "stdout":
        print("\n" + report)
    elif args.output == "file":
        output_path = (
            get_repo_root()
            / "application"
            / "sections"
            / args.section_id
            / f"evaluation_{args.mode}.md"
        )
        output_path.write_text(report)
        print(f"Evaluation saved to: {output_path}")
    elif args.output == "comment":
        # Output in format suitable for GitHub Actions to post as PR comment
        # Escape for use in GitHub Actions
        escaped = report.replace("%", "%25").replace("\n", "%0A").replace("\r", "%0D")
        print(f"::set-output name=evaluation::{escaped}")
        # Also print raw for debugging
        print("\n" + report)

    return 0


if __name__ == "__main__":
    sys.exit(main())
